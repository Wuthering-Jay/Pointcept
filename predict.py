"""
Predict script for LAS point cloud segmentation.

Pipeline:
1. Extract points to predict (based on require_labels/ignore_labels)
2. Tile input LAS files
3. Run deep learning prediction
4. Merge predicted tiles
5. Merge back excluded points
6. LAC processing (optional)

Author: Generated by GitHub Copilot
"""

import os
import sys
import shutil
import tempfile
import time
import logging
from datetime import datetime
from typing import Optional, Tuple, List
from pathlib import Path

import numpy as np
import laspy

# Fix OpenMP conflicts
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
os.environ['OMP_NUM_THREADS'] = '4'

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pointcept.engines.defaults import (
    default_config_parser,
    default_setup,
)
from pointcept.engines.test import TESTERS
from pointcept.engines.launch import launch


def setup_logger(log_dir: str) -> logging.Logger:
    """设置日志记录器，同时输出到终端和文件。"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"predict_{timestamp}.log")
    
    # 创建 logger
    logger = logging.getLogger("predict")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    
    # 文件处理器
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    file_handler.setFormatter(file_formatter)
    
    # 终端处理器
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(message)s')
    console_handler.setFormatter(console_formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    logger.info(f"日志文件: {log_file}")
    return logger


def extract_points_by_labels(
    input_path: str,
    output_predict_dir: str,
    output_excluded_dir: str,
    require_labels: Optional[List[int]] = None,
    ignore_labels: Optional[List[int]] = None,
    logger: Optional[logging.Logger] = None
) -> dict:
    """
    根据标签提取需要预测的点和需要排除的点。
    
    Args:
        input_path: 输入 LAS 文件或文件夹路径
        output_predict_dir: 输出需要预测的点的目录
        output_excluded_dir: 输出被排除的点的目录
        require_labels: 需要预测的标签列表（与 ignore_labels 互斥）
        ignore_labels: 需要忽略的标签列表（与 require_labels 互斥）
        logger: 日志记录器
    
    Returns:
        包含文件信息的字典 {filename: {"has_excluded": bool, "excluded_file": str}}
    """
    def log(msg):
        if logger:
            logger.info(msg)
        else:
            print(msg)
    
    input_path = Path(input_path)
    output_predict_dir = Path(output_predict_dir)
    output_excluded_dir = Path(output_excluded_dir)
    
    output_predict_dir.mkdir(parents=True, exist_ok=True)
    output_excluded_dir.mkdir(parents=True, exist_ok=True)
    
    # 获取所有 LAS 文件
    if input_path.is_file():
        las_files = [input_path]
    else:
        las_files = list(input_path.glob("*.las")) + list(input_path.glob("*.laz"))
    
    file_info = {}
    
    for las_file in las_files:
        log(f"处理文件: {las_file.name}")
        
        # 读取 LAS 文件
        las_data = laspy.read(las_file)
        
        # 检查是否有 classification 字段
        if not hasattr(las_data, 'classification'):
            # 没有分类字段，全部用于预测
            las_data.write(output_predict_dir / las_file.name)
            file_info[las_file.stem] = {"has_excluded": False, "excluded_file": None}
            continue
        
        labels = np.array(las_data.classification)
        
        # 确定需要预测的点和需要排除的点
        if require_labels is not None:
            # 使用 require_labels：只预测这些标签的点
            predict_mask = np.isin(labels, require_labels)
        elif ignore_labels is not None:
            # 使用 ignore_labels：排除这些标签的点
            predict_mask = ~np.isin(labels, ignore_labels)
        else:
            # 没有指定，全部预测
            las_data.write(output_predict_dir / las_file.name)
            file_info[las_file.stem] = {"has_excluded": False, "excluded_file": None}
            continue
        
        excluded_mask = ~predict_mask
        
        # 检查是否有需要排除的点
        if not np.any(excluded_mask):
            # 没有需要排除的点
            las_data.write(output_predict_dir / las_file.name)
            file_info[las_file.stem] = {"has_excluded": False, "excluded_file": None}
            continue
        
        # 检查是否有需要预测的点
        if not np.any(predict_mask):
            log(f"  警告: {las_file.name} 没有需要预测的点，跳过")
            file_info[las_file.stem] = {"has_excluded": True, "excluded_file": str(output_excluded_dir / las_file.name), "all_excluded": True}
            las_data.write(output_excluded_dir / las_file.name)
            continue
        
        log(f"  需要预测的点: {np.sum(predict_mask)}, 被排除的点: {np.sum(excluded_mask)}")
        
        # 创建预测点的 LAS 文件
        predict_las = laspy.LasData(las_data.header)
        predict_las.points = las_data.points[predict_mask]
        predict_las.update_header()
        predict_las.write(output_predict_dir / las_file.name)
        
        # 创建被排除点的 LAS 文件
        excluded_las = laspy.LasData(las_data.header)
        excluded_las.points = las_data.points[excluded_mask]
        excluded_las.update_header()
        excluded_file = output_excluded_dir / f"{las_file.stem}_excluded.las"
        excluded_las.write(excluded_file)
        
        file_info[las_file.stem] = {
            "has_excluded": True, 
            "excluded_file": str(excluded_file),
            "all_excluded": False
        }
    
    return file_info


def merge_excluded_points(
    predicted_dir: str,
    excluded_dir: str,
    output_dir: str,
    file_info: dict,
    logger: Optional[logging.Logger] = None
):
    """
    将被排除的点合并回预测结果。
    
    Args:
        predicted_dir: 预测结果目录
        excluded_dir: 被排除点的目录
        output_dir: 输出目录
        file_info: 文件信息字典
        logger: 日志记录器
    """
    def log(msg):
        if logger:
            logger.info(msg)
        else:
            print(msg)
    
    predicted_dir = Path(predicted_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 获取所有预测结果文件
    predicted_files = list(predicted_dir.glob("*.las")) + list(predicted_dir.glob("*.laz"))
    
    for pred_file in predicted_files:
        file_stem = pred_file.stem
        info = file_info.get(file_stem, {"has_excluded": False})
        
        if info.get("all_excluded", False):
            # 所有点都被排除，直接复制排除文件
            excluded_file = Path(info["excluded_file"])
            if excluded_file.exists():
                shutil.copy2(excluded_file, output_dir / f"{file_stem}.las")
            continue
        
        if not info.get("has_excluded", False):
            # 没有被排除的点，直接复制
            shutil.copy2(pred_file, output_dir / pred_file.name)
            continue
        
        excluded_file = Path(info["excluded_file"])
        if not excluded_file.exists():
            # 排除文件不存在，直接复制预测结果
            shutil.copy2(pred_file, output_dir / pred_file.name)
            continue
        
        log(f"合并文件: {pred_file.name}")
        
        # 读取预测结果和被排除的点
        pred_las = laspy.read(pred_file)
        excluded_las = laspy.read(excluded_file)
        
        # 计算总点数
        total_points = len(pred_las.points) + len(excluded_las.points)
        
        # 创建合并后的 LAS 文件
        # 使用预测结果的 header 作为基础
        merged_header = laspy.LasHeader(
            point_format=pred_las.header.point_format,
            version=pred_las.header.version
        )
        merged_header.offsets = pred_las.header.offsets
        merged_header.scales = pred_las.header.scales
        
        # 复制 VLRs
        if hasattr(pred_las.header, 'vlrs'):
            for vlr in pred_las.header.vlrs:
                merged_header.vlrs.append(vlr)
        
        merged_las = laspy.LasData(merged_header)
        merged_las.points = laspy.ScaleAwarePointRecord.zeros(total_points, header=merged_header)
        
        # 复制预测结果的点
        pred_count = len(pred_las.points)
        for dim in pred_las.point_format.dimension_names:
            merged_data = getattr(merged_las, dim)
            merged_data[:pred_count] = getattr(pred_las, dim)
        
        # 复制被排除的点
        for dim in excluded_las.point_format.dimension_names:
            if dim in pred_las.point_format.dimension_names:
                merged_data = getattr(merged_las, dim)
                merged_data[pred_count:] = getattr(excluded_las, dim)
        
        merged_las.update_header()
        merged_las.write(output_dir / pred_file.name)
        
        log(f"  合并完成: {pred_count} + {len(excluded_las.points)} = {total_points} 点")
    
    # 处理全部被排除的文件（没有预测结果）
    for file_stem, info in file_info.items():
        if info.get("all_excluded", False):
            excluded_file = Path(info["excluded_file"])
            output_file = output_dir / f"{file_stem}.las"
            if excluded_file.exists() and not output_file.exists():
                shutil.copy2(excluded_file, output_file)
                log(f"复制全排除文件: {file_stem}.las")


def run_prediction(cfg):
    """Run deep learning prediction."""
    cfg = default_setup(cfg)
    tester = TESTERS.build(dict(type=cfg.test.type, cfg=cfg))
    tester.test()


def predict_las(
    input_dir: str,
    output_dir: str,
    config_file: str,
    weight_file: str,
    # Label filter parameters
    require_labels: Optional[List[int]] = None,
    ignore_labels: Optional[List[int]] = None,
    # Tile parameters
    window_size: Tuple[float, float] = (200.0, 200.0),
    min_points: Optional[int] = 5000,
    max_points: Optional[int] = None,
    # Merge parameters
    label_remap_file: Optional[str] = None,
    # LAC parameters
    use_lac: bool = False,
    lac_dll_path: str = r"libs\Release\LiDAROprationDLLEx.dll",
    # GPU parameters
    num_gpus: int = 1,
):
    """
    完整的 LAS 点云预测流程。
    
    Args:
        input_dir: 输入 LAS 文件夹路径
        output_dir: 最终输出文件夹路径
        config_file: 配置文件路径
        weight_file: 模型权重文件路径
        
        # 标签过滤参数（二者互斥，不能同时启用）
        require_labels: 需要预测的标签列表，其他标签的点会被提取出来最后合并回去
        ignore_labels: 需要忽略的标签列表，这些标签的点会被提取出来最后合并回去
        
        # Tile 参数
        window_size: tile 窗口大小，默认 (200, 200)
        min_points: tile 最小点数阈值，默认 5000
        max_points: tile 最大点数阈值，默认 None
        
        # Merge 参数
        label_remap_file: 标签重映射文件路径，默认 None
        
        # LAC 参数
        use_lac: 是否进行 LAC 处理，默认 False
        lac_dll_path: LAC DLL 文件路径
        
        # GPU 参数
        num_gpus: GPU 数量，默认 1
    """
    # 验证参数
    if require_labels is not None and ignore_labels is not None:
        raise ValueError("require_labels 和 ignore_labels 不能同时启用！")
    
    # 设置日志
    weight_dir = os.path.dirname(weight_file)
    logger = setup_logger(weight_dir)
    
    # 记录开始时间
    total_start_time = time.time()
    
    logger.info("="*60)
    logger.info("LAS 点云预测流程开始")
    logger.info("="*60)
    logger.info(f"输入目录: {input_dir}")
    logger.info(f"输出目录: {output_dir}")
    logger.info(f"配置文件: {config_file}")
    logger.info(f"权重文件: {weight_file}")
    logger.info(f"require_labels: {require_labels}")
    logger.info(f"ignore_labels: {ignore_labels}")
    logger.info(f"window_size: {window_size}")
    logger.info(f"min_points: {min_points}")
    logger.info(f"use_lac: {use_lac}")
    logger.info("")
    
    from utils.las_tile import process_las_tiles
    from utils.las_merge import merge_las_segments
    
    # 创建临时目录
    temp_extract_predict_dir = tempfile.mkdtemp(prefix="predict_extract_")
    temp_extract_excluded_dir = tempfile.mkdtemp(prefix="predict_excluded_")
    temp_tile_dir = tempfile.mkdtemp(prefix="predict_tile_")
    temp_pred_dir = tempfile.mkdtemp(prefix="predict_result_")
    temp_merge_dir = tempfile.mkdtemp(prefix="predict_merge_")
    
    if use_lac:
        temp_final_merge_dir = tempfile.mkdtemp(prefix="predict_final_merge_")
    else:
        temp_final_merge_dir = output_dir
    
    file_info = {}
    
    try:
        # ============ Step 1: 提取需要预测的点 ============
        if require_labels is not None or ignore_labels is not None:
            logger.info("\n" + "="*60)
            logger.info("Step 1: 提取需要预测的点")
            logger.info("="*60)
            
            file_info = extract_points_by_labels(
                input_path=input_dir,
                output_predict_dir=temp_extract_predict_dir,
                output_excluded_dir=temp_extract_excluded_dir,
                require_labels=require_labels,
                ignore_labels=ignore_labels,
                logger=logger
            )
            
            tile_input_dir = temp_extract_predict_dir
        else:
            tile_input_dir = input_dir
        
        # ============ Step 2: Tile 分块 ============
        logger.info("\n" + "="*60)
        logger.info("Step 2: Tile 分块处理")
        logger.info("="*60)
        
        process_las_tiles(
            input_path=tile_input_dir,
            output_dir=temp_tile_dir,
            window_size=window_size,
            min_points=min_points,
            max_points=max_points,
            label_remap=False,
            label_count=False,
            save_sample_weight=False,
            require_labels=None,  # 不在 tile 阶段过滤
            use_trash_bin=False,
            trash_bin_label=0
        )
        
        # ============ Step 3: 深度学习预测 ============
        logger.info("\n" + "="*60)
        logger.info("Step 3: 深度学习预测")
        logger.info("="*60)
        
        # 复制 tile 文件到预测目录（因为预测会原地修改文件）
        os.makedirs(temp_pred_dir, exist_ok=True)
        for f in os.listdir(temp_tile_dir):
            if f.lower().endswith('.las'):
                shutil.copy2(
                    os.path.join(temp_tile_dir, f),
                    os.path.join(temp_pred_dir, f)
                )
        
        # 获取项目根目录
        project_root = os.path.dirname(os.path.abspath(__file__))
        
        # 将配置文件路径转为绝对路径
        if not os.path.isabs(config_file):
            config_file_abs = os.path.join(project_root, config_file)
        else:
            config_file_abs = config_file
        
        # 处理配置文件中的 _base_ 路径问题
        temp_config_dir = os.path.join(project_root, "configs", "_temp_predict_")
        os.makedirs(temp_config_dir, exist_ok=True)
        temp_config_file = os.path.join(temp_config_dir, os.path.basename(config_file_abs))
        
        with open(config_file_abs, 'r', encoding='utf-8') as f:
            config_content = f.read()
        
        with open(temp_config_file, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        # 保存当前工作目录
        original_cwd = os.getcwd()
        
        # 切换到项目根目录
        os.chdir(project_root)
        
        try:
            options = {
                "weight": weight_file,
                "data_root": temp_pred_dir,
                "data.test.data_root": temp_pred_dir,
                "data.test.split": "",
            }
            
            cfg = default_config_parser(temp_config_file, options)
            
            launch(
                run_prediction,
                num_gpus_per_machine=num_gpus,
                num_machines=1,
                machine_rank=0,
                dist_url="auto",
                cfg=(cfg,),
            )
        finally:
            os.chdir(original_cwd)
            if os.path.exists(temp_config_dir):
                shutil.rmtree(temp_config_dir)
        
        # ============ Step 4: Merge 合并 tile ============
        logger.info("\n" + "="*60)
        logger.info("Step 4: Merge 合并 tile 结果")
        logger.info("="*60)
        
        os.makedirs(temp_merge_dir, exist_ok=True)
        
        merge_las_segments(
            input_path=temp_pred_dir,
            output_dir=temp_merge_dir,
            label_remap_file=label_remap_file
        )
        
        # ============ Step 5: 合并被排除的点 ============
        if require_labels is not None or ignore_labels is not None:
            logger.info("\n" + "="*60)
            logger.info("Step 5: 合并被排除的点")
            logger.info("="*60)
            
            merge_excluded_points(
                predicted_dir=temp_merge_dir,
                excluded_dir=temp_extract_excluded_dir,
                output_dir=temp_final_merge_dir,
                file_info=file_info,
                logger=logger
            )
        else:
            # 没有排除点，直接复制
            if temp_final_merge_dir != output_dir:
                os.makedirs(temp_final_merge_dir, exist_ok=True)
                for f in os.listdir(temp_merge_dir):
                    if f.lower().endswith('.las'):
                        shutil.copy2(
                            os.path.join(temp_merge_dir, f),
                            os.path.join(temp_final_merge_dir, f)
                        )
            else:
                # temp_final_merge_dir == output_dir，需要复制文件
                os.makedirs(output_dir, exist_ok=True)
                for f in os.listdir(temp_merge_dir):
                    if f.lower().endswith('.las'):
                        shutil.copy2(
                            os.path.join(temp_merge_dir, f),
                            os.path.join(output_dir, f)
                        )
        
        # ============ Step 6: LAC 处理 (可选) ============
        if use_lac:
            logger.info("\n" + "="*60)
            logger.info("Step 6: LAC 处理")
            logger.info("="*60)
            
            from utils.tin import batch_lac_process
            
            os.makedirs(output_dir, exist_ok=True)
            
            batch_lac_process(
                input_dir=temp_final_merge_dir,
                output_dir=output_dir,
                use_tile=True,
                window_size=(1000.0, 1000.0),
                min_points=10000,
                dll_path=lac_dll_path
            )
        
        # 计算总时间
        total_time = time.time() - total_start_time
        
        logger.info("\n" + "="*60)
        logger.info(f"预测完成！")
        logger.info(f"总耗时: {total_time:.2f} 秒 ({total_time/60:.2f} 分钟)")
        logger.info(f"结果保存在: {output_dir}")
        logger.info("="*60)
        
    finally:
        # 清理临时目录
        logger.info("\n>>> 清理临时文件...")
        for temp_dir in [temp_extract_predict_dir, temp_extract_excluded_dir, 
                         temp_tile_dir, temp_pred_dir, temp_merge_dir]:
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
        if use_lac and os.path.exists(temp_final_merge_dir):
            shutil.rmtree(temp_final_merge_dir)


if __name__ == "__main__":
    # 示例用法
    INPUT_DIR = r"E:\data\补充测试数据\城市\青白江 试验数据.las"
    OUTPUT_DIR = r"E:\data\云南遥感中心\第二批\disk01\val_pred"

    CONFIG_FILE = r"E:\code\python\Pointcept\ckpt\semseg-pt-v2m5-0-base.py"
    WEIGHT_FILE = r"E:\code\python\Pointcept\ckpt\model_best.pth"
    LABEL_REMAP_FILE = r"E:\code\python\Pointcept\ckpt\label_mapping.json"
    
    predict_las(
        input_dir=INPUT_DIR,
        output_dir=OUTPUT_DIR,
        config_file=CONFIG_FILE,
        weight_file=WEIGHT_FILE,
        # 标签过滤参数（二者互斥）
        require_labels=None,  # 需要预测的标签列表
        ignore_labels=[15],   # 需要忽略的标签列表
        # Tile 参数
        window_size=(200.0, 200.0),
        min_points=5000,
        # Merge 参数
        label_remap_file=LABEL_REMAP_FILE if LABEL_REMAP_FILE else None,
        # LAC 参数
        use_lac=True,
    )
