"""
Predict script for LAS point cloud segmentation.

Pipeline (per file):
1. Extract points to predict (based on require_labels/ignore_labels)
2. Tile input LAS files
3. Run deep learning prediction (with optional AMP)
4. Merge predicted tiles
5. Merge back excluded points
6. LAC processing (optional)
7. Clean up cache

Author: Generated by GitHub Copilot
"""
import utils.numpy_compat_1to2  # Ensure NumPy 2.x compatibility in NumPy 1.x environments
import os
import sys
import shutil
import tempfile
import time
import logging
import gc
from datetime import datetime
from typing import Optional, Tuple, List
from pathlib import Path

import numpy as np
import laspy
import torch

# Fix OpenMP conflicts
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
os.environ['OMP_NUM_THREADS'] = '4'

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pointcept.engines.defaults import (
    default_config_parser,
    default_setup,
)
from pointcept.engines.test import TESTERS
from pointcept.engines.launch import launch


def cleanup_cache():
    """清理 Python 垃圾回收和 PyTorch CUDA 缓存。"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


def setup_logger(log_dir: str) -> logging.Logger:
    """设置日志记录器，同时输出到终端和文件。"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"predict_{timestamp}.log")
    
    # 创建 logger
    logger = logging.getLogger("predict")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    
    # 文件处理器
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter('%(asctime)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    file_handler.setFormatter(file_formatter)
    
    # 终端处理器
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(message)s')
    console_handler.setFormatter(console_formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    logger.info(f"日志文件: {log_file}")
    return logger


def extract_points_by_labels(
    input_path: str,
    output_predict_dir: str,
    output_excluded_dir: str,
    require_labels: Optional[List[int]] = None,
    ignore_labels: Optional[List[int]] = None,
    logger: Optional[logging.Logger] = None
) -> dict:
    """
    根据标签提取需要预测的点和需要排除的点。
    
    Args:
        input_path: 输入 LAS 文件或文件夹路径
        output_predict_dir: 输出需要预测的点的目录
        output_excluded_dir: 输出被排除的点的目录
        require_labels: 需要预测的标签列表（与 ignore_labels 互斥）
        ignore_labels: 需要忽略的标签列表（与 require_labels 互斥）
        logger: 日志记录器
    
    Returns:
        包含文件信息的字典 {filename: {"has_excluded": bool, "excluded_file": str}}
    """
    def log(msg):
        if logger:
            logger.info(msg)
        else:
            print(msg)
    
    input_path = Path(input_path)
    output_predict_dir = Path(output_predict_dir)
    output_excluded_dir = Path(output_excluded_dir)
    
    output_predict_dir.mkdir(parents=True, exist_ok=True)
    output_excluded_dir.mkdir(parents=True, exist_ok=True)
    
    # 获取所有 LAS 文件
    if input_path.is_file():
        las_files = [input_path]
    else:
        las_files = list(input_path.glob("*.las")) + list(input_path.glob("*.laz"))
    
    file_info = {}
    
    for las_file in las_files:
        log(f"处理文件: {las_file.name}")
        
        # 读取 LAS 文件
        las_data = laspy.read(las_file)
        
        # 检查是否有 classification 字段
        if not hasattr(las_data, 'classification'):
            # 没有分类字段，全部用于预测
            las_data.write(output_predict_dir / las_file.name)
            file_info[las_file.stem] = {"has_excluded": False, "excluded_file": None}
            continue
        
        labels = np.array(las_data.classification)
        
        # 确定需要预测的点和需要排除的点
        if require_labels is not None:
            # 使用 require_labels：只预测这些标签的点
            predict_mask = np.isin(labels, require_labels)
        elif ignore_labels is not None:
            # 使用 ignore_labels：排除这些标签的点
            predict_mask = ~np.isin(labels, ignore_labels)
        else:
            # 没有指定，全部预测
            las_data.write(output_predict_dir / las_file.name)
            file_info[las_file.stem] = {"has_excluded": False, "excluded_file": None}
            continue
        
        excluded_mask = ~predict_mask
        
        # 检查是否有需要排除的点
        if not np.any(excluded_mask):
            # 没有需要排除的点
            las_data.write(output_predict_dir / las_file.name)
            file_info[las_file.stem] = {"has_excluded": False, "excluded_file": None}
            continue
        
        # 检查是否有需要预测的点
        if not np.any(predict_mask):
            log(f"  警告: {las_file.name} 没有需要预测的点，跳过")
            file_info[las_file.stem] = {"has_excluded": True, "excluded_file": str(output_excluded_dir / las_file.name), "all_excluded": True}
            las_data.write(output_excluded_dir / las_file.name)
            continue
        
        log(f"  需要预测的点: {np.sum(predict_mask)}, 被排除的点: {np.sum(excluded_mask)}")
        
        # 创建预测点的 LAS 文件
        predict_las = laspy.LasData(las_data.header)
        predict_las.points = las_data.points[predict_mask]
        predict_las.update_header()
        predict_las.write(output_predict_dir / las_file.name)
        
        # 创建被排除点的 LAS 文件
        excluded_las = laspy.LasData(las_data.header)
        excluded_las.points = las_data.points[excluded_mask]
        excluded_las.update_header()
        excluded_file = output_excluded_dir / f"{las_file.stem}_excluded.las"
        excluded_las.write(excluded_file)
        
        file_info[las_file.stem] = {
            "has_excluded": True, 
            "excluded_file": str(excluded_file),
            "all_excluded": False
        }
    
    return file_info


def merge_excluded_points(
    predicted_dir: str,
    excluded_dir: str,
    output_dir: str,
    file_info: dict,
    logger: Optional[logging.Logger] = None
):
    """
    将被排除的点合并回预测结果。
    
    Args:
        predicted_dir: 预测结果目录
        excluded_dir: 被排除点的目录
        output_dir: 输出目录
        file_info: 文件信息字典
        logger: 日志记录器
    """
    def log(msg):
        if logger:
            logger.info(msg)
        else:
            print(msg)
    
    predicted_dir = Path(predicted_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 获取所有预测结果文件
    predicted_files = list(predicted_dir.glob("*.las")) + list(predicted_dir.glob("*.laz"))
    
    for pred_file in predicted_files:
        file_stem = pred_file.stem
        info = file_info.get(file_stem, {"has_excluded": False})
        
        if info.get("all_excluded", False):
            # 所有点都被排除，直接复制排除文件
            excluded_file = Path(info["excluded_file"])
            if excluded_file.exists():
                shutil.copy2(excluded_file, output_dir / f"{file_stem}.las")
            continue
        
        if not info.get("has_excluded", False):
            # 没有被排除的点，直接复制
            shutil.copy2(pred_file, output_dir / pred_file.name)
            continue
        
        excluded_file = Path(info["excluded_file"])
        if not excluded_file.exists():
            # 排除文件不存在，直接复制预测结果
            shutil.copy2(pred_file, output_dir / pred_file.name)
            continue
        
        log(f"合并文件: {pred_file.name}")
        
        # 读取预测结果和被排除的点
        pred_las = laspy.read(pred_file)
        excluded_las = laspy.read(excluded_file)
        
        # 计算总点数
        total_points = len(pred_las.points) + len(excluded_las.points)
        
        # 创建合并后的 LAS 文件
        # 使用预测结果的 header 作为基础
        merged_header = laspy.LasHeader(
            point_format=pred_las.header.point_format,
            version=pred_las.header.version
        )
        merged_header.offsets = pred_las.header.offsets
        merged_header.scales = pred_las.header.scales
        
        # 复制 VLRs
        if hasattr(pred_las.header, 'vlrs'):
            for vlr in pred_las.header.vlrs:
                merged_header.vlrs.append(vlr)
        
        merged_las = laspy.LasData(merged_header)
        merged_las.points = laspy.ScaleAwarePointRecord.zeros(total_points, header=merged_header)
        
        # 复制预测结果的点
        pred_count = len(pred_las.points)
        for dim in pred_las.point_format.dimension_names:
            merged_data = getattr(merged_las, dim)
            merged_data[:pred_count] = getattr(pred_las, dim)
        
        # 复制被排除的点
        for dim in excluded_las.point_format.dimension_names:
            if dim in pred_las.point_format.dimension_names:
                merged_data = getattr(merged_las, dim)
                merged_data[pred_count:] = getattr(excluded_las, dim)
        
        merged_las.update_header()
        merged_las.write(output_dir / pred_file.name)
        
        log(f"  合并完成: {pred_count} + {len(excluded_las.points)} = {total_points} 点")
    
    # 处理全部被排除的文件（没有预测结果）
    for file_stem, info in file_info.items():
        if info.get("all_excluded", False):
            excluded_file = Path(info["excluded_file"])
            output_file = output_dir / f"{file_stem}.las"
            if excluded_file.exists() and not output_file.exists():
                shutil.copy2(excluded_file, output_file)
                log(f"复制全排除文件: {file_stem}.las")


def run_prediction(cfg):
    """Run deep learning prediction."""
    cfg = default_setup(cfg)
    tester = TESTERS.build(dict(type=cfg.test.type, cfg=cfg))
    tester.test()


def get_las_files(input_path: str, recursive: bool = False) -> List[Path]:
    """
    获取输入路径下的所有 LAS 文件。
    
    Args:
        input_path: 输入文件或文件夹路径
        recursive: 是否递归搜索子文件夹
    
    Returns:
        LAS 文件路径列表
    """
    input_path = Path(input_path)
    
    if input_path.is_file():
        if input_path.suffix.lower() in ['.las', '.laz']:
            return [input_path]
        return []
    
    if recursive:
        # 递归搜索所有子文件夹
        las_files = list(input_path.rglob("*.las")) + list(input_path.rglob("*.laz"))
    else:
        # 只搜索当前文件夹
        las_files = list(input_path.glob("*.las")) + list(input_path.glob("*.laz"))
    
    return sorted(las_files)


def process_single_las(
    las_file: Path,
    output_file: Path,
    config_file: str,
    weight_file: str,
    require_labels: Optional[List[int]] = None,
    ignore_labels: Optional[List[int]] = None,
    window_size: Tuple[float, float] = (200.0, 200.0),
    min_points: Optional[int] = 5000,
    max_points: Optional[int] = None,
    label_remap_file: Optional[str] = None,
    use_lac: bool = False,
    lac_dll_path: str = r"libs\Release\LiDAROprationDLLEx.dll",
    num_gpus: int = 1,
    enable_amp: bool = False,
    cache_cleanup_interval: Optional[int] = None,
    temp_dir: Optional[str] = None,
    logger: Optional[logging.Logger] = None,
):
    """
    处理单个 LAS 文件的完整预测流程。
    
    Args:
        las_file: 输入 LAS 文件路径
        output_file: 输出 LAS 文件路径
        config_file: 配置文件路径
        weight_file: 模型权重文件路径
        require_labels: 需要预测的标签列表
        ignore_labels: 需要忽略的标签列表
        window_size: tile 窗口大小
        min_points: tile 最小点数阈值
        max_points: tile 最大点数阈值
        label_remap_file: 标签重映射文件路径
        use_lac: 是否进行 LAC 处理
        lac_dll_path: LAC DLL 文件路径
        num_gpus: GPU 数量
        enable_amp: 是否启用 AMP 混合精度
        cache_cleanup_interval: 每隔多少个 test 步骤清理一次 CUDA 缓存
        temp_dir: 临时文件目录，默认为 None 使用输出文件所在目录
        logger: 日志记录器
    """
    def log(msg):
        if logger:
            logger.info(msg)
        else:
            print(msg)
    
    from utils.las_tile import process_las_tiles
    from utils.las_merge import merge_las_segments
    
    # 确定临时目录的基础路径（默认使用输出文件所在目录）
    if temp_dir is None:
        temp_base_dir = str(output_file.parent)
    else:
        temp_base_dir = temp_dir
    os.makedirs(temp_base_dir, exist_ok=True)
    
    # 创建临时目录（在指定目录下创建）
    temp_extract_predict_dir = tempfile.mkdtemp(prefix="predict_extract_", dir=temp_base_dir)
    temp_extract_excluded_dir = tempfile.mkdtemp(prefix="predict_excluded_", dir=temp_base_dir)
    temp_tile_dir = tempfile.mkdtemp(prefix="predict_tile_", dir=temp_base_dir)
    temp_pred_dir = tempfile.mkdtemp(prefix="predict_result_", dir=temp_base_dir)
    temp_merge_dir = tempfile.mkdtemp(prefix="predict_merge_", dir=temp_base_dir)
    temp_final_merge_dir = tempfile.mkdtemp(prefix="predict_final_merge_", dir=temp_base_dir) if use_lac else None
    
    file_info = {}
    
    try:
        # ============ Step 1: 提取需要预测的点 ============
        if require_labels is not None or ignore_labels is not None:
            log(f"  [Step 1] 提取需要预测的点...")
            
            file_info = extract_points_by_labels(
                input_path=str(las_file),
                output_predict_dir=temp_extract_predict_dir,
                output_excluded_dir=temp_extract_excluded_dir,
                require_labels=require_labels,
                ignore_labels=ignore_labels,
                logger=None  # 不重复日志
            )
            
            tile_input_dir = temp_extract_predict_dir
        else:
            tile_input_dir = str(las_file.parent)
            # 复制单个文件到临时目录
            temp_single_dir = tempfile.mkdtemp(prefix="predict_single_")
            shutil.copy2(las_file, os.path.join(temp_single_dir, las_file.name))
            tile_input_dir = temp_single_dir
        
        # 检查是否所有点都被排除
        file_stem = las_file.stem
        if file_info.get(file_stem, {}).get("all_excluded", False):
            log(f"  所有点都被排除，直接复制原文件")
            shutil.copy2(las_file, output_file)
            return
        
        # ============ Step 2: Tile 分块 ============
        log(f"  [Step 2] Tile 分块处理...")
        
        if require_labels is not None or ignore_labels is not None:
            process_las_tiles(
                input_path=tile_input_dir,
                output_dir=temp_tile_dir,
                window_size=window_size,
                min_points=min_points,
                max_points=max_points,
                label_remap=False,
                label_count=False,
                save_sample_weight=False,
                require_labels=None,
                use_trash_bin=False,
                trash_bin_label=0
            )
        else:
            process_las_tiles(
                input_path=tile_input_dir,
                output_dir=temp_tile_dir,
                window_size=window_size,
                min_points=min_points,
                max_points=max_points,
                label_remap=False,
                label_count=False,
                save_sample_weight=False,
                require_labels=None,
                use_trash_bin=False,
                trash_bin_label=0
            )
        
        # ============ Step 3: 深度学习预测 ============
        log(f"  [Step 3] 深度学习预测...")
        
        # 复制 tile 文件到预测目录
        os.makedirs(temp_pred_dir, exist_ok=True)
        for f in os.listdir(temp_tile_dir):
            if f.lower().endswith('.las'):
                shutil.copy2(
                    os.path.join(temp_tile_dir, f),
                    os.path.join(temp_pred_dir, f)
                )
        
        # 获取项目根目录
        project_root = os.path.dirname(os.path.abspath(__file__))
        
        # 将配置文件路径转为绝对路径
        if not os.path.isabs(config_file):
            config_file_abs = os.path.join(project_root, config_file)
        else:
            config_file_abs = config_file
        
        # 处理配置文件中的 _base_ 路径问题
        temp_config_dir = os.path.join(project_root, "configs", "_temp_predict_")
        os.makedirs(temp_config_dir, exist_ok=True)
        temp_config_file = os.path.join(temp_config_dir, os.path.basename(config_file_abs))
        
        with open(config_file_abs, 'r', encoding='utf-8') as f:
            config_content = f.read()
        
        with open(temp_config_file, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        # 保存当前工作目录
        original_cwd = os.getcwd()
        
        # 切换到项目根目录
        os.chdir(project_root)
        
        try:
            options = {
                "weight": weight_file,
                "data_root": temp_pred_dir,
                "data.test.data_root": temp_pred_dir,
                "data.test.split": "",
                "enable_amp": enable_amp,
                "cache_cleanup_interval": cache_cleanup_interval,
            }
            
            cfg = default_config_parser(temp_config_file, options)
            
            launch(
                run_prediction,
                num_gpus_per_machine=num_gpus,
                num_machines=1,
                machine_rank=0,
                dist_url="auto",
                cfg=(cfg,),
            )
        finally:
            os.chdir(original_cwd)
            if os.path.exists(temp_config_dir):
                shutil.rmtree(temp_config_dir)
        
        # ============ Step 4: Merge 合并 tile ============
        log(f"  [Step 4] 合并 tile 结果...")
        
        os.makedirs(temp_merge_dir, exist_ok=True)
        
        merge_las_segments(
            input_path=temp_pred_dir,
            output_dir=temp_merge_dir,
            label_remap_file=label_remap_file
        )
        
        # ============ Step 5: 合并被排除的点 ============
        if require_labels is not None or ignore_labels is not None:
            if file_info.get(file_stem, {}).get("has_excluded", False):
                log(f"  [Step 5] 合并被排除的点...")
                
                merge_excluded_points(
                    predicted_dir=temp_merge_dir,
                    excluded_dir=temp_extract_excluded_dir,
                    output_dir=temp_final_merge_dir if use_lac else str(output_file.parent),
                    file_info=file_info,
                    logger=None
                )
                
                # 如果不使用 LAC，需要重命名输出文件
                if not use_lac:
                    merged_file = output_file.parent / f"{file_stem}.las"
                    if merged_file.exists() and merged_file != output_file:
                        shutil.move(merged_file, output_file)
            else:
                # 没有排除点
                for f in os.listdir(temp_merge_dir):
                    if f.lower().endswith('.las'):
                        if use_lac:
                            shutil.copy2(os.path.join(temp_merge_dir, f), temp_final_merge_dir)
                        else:
                            shutil.copy2(os.path.join(temp_merge_dir, f), output_file)
                        break
        else:
            # 没有标签过滤
            for f in os.listdir(temp_merge_dir):
                if f.lower().endswith('.las'):
                    if use_lac:
                        os.makedirs(temp_final_merge_dir, exist_ok=True)
                        shutil.copy2(os.path.join(temp_merge_dir, f), temp_final_merge_dir)
                    else:
                        shutil.copy2(os.path.join(temp_merge_dir, f), output_file)
                    break
        
        # ============ Step 6: LAC 处理 (可选) ============
        if use_lac:
            log(f"  [Step 6] LAC 处理...")
            
            from utils.tin import batch_lac_process
            
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            # LAC 处理后直接输出到目标文件
            temp_lac_output = tempfile.mkdtemp(prefix="predict_lac_", dir=temp_base_dir)
            
            batch_lac_process(
                input_dir=temp_final_merge_dir,
                output_dir=temp_lac_output,
                use_tile=True,
                window_size=(1000.0, 1000.0),
                min_points=10000,
                dll_path=lac_dll_path
            )
            
            # 复制 LAC 处理结果到输出文件
            for f in os.listdir(temp_lac_output):
                if f.lower().endswith('.las'):
                    shutil.copy2(os.path.join(temp_lac_output, f), output_file)
                    break
            
            shutil.rmtree(temp_lac_output)
        
    finally:
        # 清理临时目录
        for temp_dir in [temp_extract_predict_dir, temp_extract_excluded_dir, 
                         temp_tile_dir, temp_pred_dir, temp_merge_dir]:
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
        if temp_final_merge_dir and os.path.exists(temp_final_merge_dir):
            shutil.rmtree(temp_final_merge_dir)
        # 清理单文件临时目录
        if 'temp_single_dir' in locals() and os.path.exists(temp_single_dir):
            shutil.rmtree(temp_single_dir)


def predict_las(
    input_dir: str,
    output_dir: str,
    config_file: str,
    weight_file: str,
    # Label filter parameters
    require_labels: Optional[List[int]] = None,
    ignore_labels: Optional[List[int]] = None,
    # Tile parameters
    window_size: Tuple[float, float] = (200.0, 200.0),
    min_points: Optional[int] = 5000,
    max_points: Optional[int] = None,
    # Merge parameters
    label_remap_file: Optional[str] = None,
    # LAC parameters
    use_lac: bool = False,
    lac_dll_path: str = r"libs\Release\LiDAROprationDLLEx.dll",
    # GPU parameters
    num_gpus: int = 1,
    # New parameters
    recursive: bool = False,
    enable_amp: bool = False,
    cache_cleanup_interval: Optional[int] = None,
):
    """
    完整的 LAS 点云预测流程（逐文件处理）。
    
    Args:
        input_dir: 输入 LAS 文件或文件夹路径
        output_dir: 最终输出文件夹路径
        config_file: 配置文件路径
        weight_file: 模型权重文件路径
        
        # 标签过滤参数（二者互斥，不能同时启用）
        require_labels: 需要预测的标签列表，其他标签的点会被提取出来最后合并回去
        ignore_labels: 需要忽略的标签列表，这些标签的点会被提取出来最后合并回去
        
        # Tile 参数
        window_size: tile 窗口大小，默认 (200, 200)
        min_points: tile 最小点数阈值，默认 5000
        max_points: tile 最大点数阈值，默认 None
        
        # Merge 参数
        label_remap_file: 标签重映射文件路径，默认 None
        
        # LAC 参数
        use_lac: 是否进行 LAC 处理，默认 False
        lac_dll_path: LAC DLL 文件路径
        
        # GPU 参数
        num_gpus: GPU 数量，默认 1
        
        # 新增参数
        recursive: 是否递归处理子文件夹中的 LAS 文件，默认 False
        enable_amp: 是否启用 AMP 混合精度预测（更快），默认 False
        cache_cleanup_interval: 每隔多少个文件进行一次 torch cache 清理，默认 None 不进行定期清理
    """
    # 验证参数
    if require_labels is not None and ignore_labels is not None:
        raise ValueError("require_labels 和 ignore_labels 不能同时启用！")
    
    # 设置日志
    weight_dir = os.path.dirname(weight_file)
    logger = setup_logger(weight_dir)
    
    # 记录开始时间
    total_start_time = time.time()
    
    logger.info("="*60)
    logger.info("LAS 点云预测流程开始（逐文件处理模式）")
    logger.info("="*60)
    logger.info(f"输入路径: {input_dir}")
    logger.info(f"输出目录: {output_dir}")
    logger.info(f"临时文件目录: {output_dir}")
    logger.info(f"配置文件: {config_file}")
    logger.info(f"权重文件: {weight_file}")
    logger.info(f"递归处理子文件夹: {recursive}")
    logger.info(f"AMP 混合精度: {enable_amp}")
    logger.info(f"定期缓存清理间隔(test步数): {cache_cleanup_interval}")
    logger.info(f"require_labels: {require_labels}")
    logger.info(f"ignore_labels: {ignore_labels}")
    logger.info(f"window_size: {window_size}")
    logger.info(f"min_points: {min_points}")
    logger.info(f"use_lac: {use_lac}")
    logger.info("")
    
    # 获取所有 LAS 文件
    input_path = Path(input_dir)
    las_files = get_las_files(input_dir, recursive=recursive)
    
    if not las_files:
        logger.error(f"未找到 LAS 文件: {input_dir}")
        return
    
    logger.info(f"找到 {len(las_files)} 个 LAS 文件待处理")
    logger.info("")
    
    # 创建输出目录
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # 统计
    success_count = 0
    fail_count = 0
    failed_files = []
    
    # 逐个文件处理
    for i, las_file in enumerate(las_files, 1):
        file_start_time = time.time()
        
        # 计算相对路径，保持目录结构
        if input_path.is_file():
            relative_path = las_file.name
        else:
            try:
                relative_path = las_file.relative_to(input_path)
            except ValueError:
                relative_path = las_file.name
        
        output_file = output_path / relative_path
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        logger.info("="*60)
        logger.info(f"[{i}/{len(las_files)}] 处理文件: {las_file.name}")
        logger.info(f"  输入: {las_file}")
        logger.info(f"  输出: {output_file}")
        logger.info("="*60)
        
        try:
            process_single_las(
                las_file=las_file,
                output_file=output_file,
                config_file=config_file,
                weight_file=weight_file,
                require_labels=require_labels,
                ignore_labels=ignore_labels,
                window_size=window_size,
                min_points=min_points,
                max_points=max_points,
                label_remap_file=label_remap_file,
                use_lac=use_lac,
                lac_dll_path=lac_dll_path,
                num_gpus=num_gpus,
                enable_amp=enable_amp,
                cache_cleanup_interval=cache_cleanup_interval,
                temp_dir=str(output_path),  # 临时文件放在输出目录
                logger=logger,
            )
            
            file_time = time.time() - file_start_time
            logger.info(f"  ✓ 完成! 耗时: {file_time:.2f} 秒")
            success_count += 1
            
        except Exception as e:
            logger.error(f"  ✗ 处理失败: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            fail_count += 1
            failed_files.append(str(las_file))
        
        logger.info("")
    
    # 计算总时间
    total_time = time.time() - total_start_time
    
    logger.info("\n" + "="*60)
    logger.info("预测完成!")
    logger.info("="*60)
    logger.info(f"总文件数: {len(las_files)}")
    logger.info(f"成功: {success_count}")
    logger.info(f"失败: {fail_count}")
    if failed_files:
        logger.info(f"失败文件列表:")
        for f in failed_files:
            logger.info(f"  - {f}")
    logger.info(f"总耗时: {total_time:.2f} 秒 ({total_time/60:.2f} 分钟)")
    logger.info(f"结果保存在: {output_dir}")
    logger.info("="*60)


if __name__ == "__main__":
    # 示例用法
    INPUT_DIR = r"E:\data\云南遥感中心数据第二批\city\新建文件夹"
    OUTPUT_DIR = r"E:\data\云南遥感中心数据第二批\city\processed_城镇12_pred"

    CONFIG_FILE = r"ckpt\other\semseg-pt-v2m5-0-base.py"
    WEIGHT_FILE = r"E:\code\python\Pointcept\ckpt\other\model_best.pth"
    LABEL_REMAP_FILE = r"E:\code\python\Pointcept\ckpt\city\label_mapping.json"
    
    predict_las(
        input_dir=INPUT_DIR,
        output_dir=OUTPUT_DIR,
        config_file=CONFIG_FILE,
        weight_file=WEIGHT_FILE,
        # 标签过滤参数（二者互斥）
        require_labels=None,  # 需要预测的标签列表
        ignore_labels=[],     # 需要忽略的标签列表
        # Tile 参数
        window_size=(200.0, 200.0),
        min_points=5000,
        # Merge 参数
        label_remap_file=LABEL_REMAP_FILE if LABEL_REMAP_FILE else None,
        # LAC 参数
        use_lac=True,
        # 新增参数
        recursive=False,      # 是否递归处理子文件夹
        enable_amp=True,      # 是否启用 AMP 混合精度（更快）
        cache_cleanup_interval=10,  # 每隔多少个文件清理一次 torch cache，None 表示不定期清理
    )
